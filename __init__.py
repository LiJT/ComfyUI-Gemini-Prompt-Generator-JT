import os
import json
import google.generativeai as genai
import random
import re 
import concurrent.futures
import threading
import time  # æ·»åŠ timeæ¨¡å—ç”¨äºè¶…æ—¶å¤„ç†
from collections import deque

class ComfyUITimeoutError(Exception):
    pass

class ComfyUIAPIError(Exception):
    pass

# ä½¿ç”¨dequeæ›¿ä»£listï¼Œå¹¶è®¾ç½®æœ€å¤§é•¿åº¦ä¸º15
prompt_history = deque(maxlen=15)  # å°†memoryé‡å‘½åä¸ºprompt_history
last_memory_mode = None  # æ·»åŠ ä¸€ä¸ªå˜é‡æ¥è·Ÿè¸ªä¸Šä¸€æ¬¡çš„å†…å­˜æ¨¡å¼

# æ–°å¢å‡½æ•°: ä» config.json æ–‡ä»¶ä¸­è¯»å– API å¯†é’¥
def get_gemini_api_key():
    try:
        config_path = os.path.join(os.path.dirname(__file__), 'config.json')  # ç¡®ä¿è·¯å¾„æ˜¯ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„
        with open(config_path, 'r') as f:
            config = json.load(f)
        api_key = config.get("GEMINI_API_KEY", "").strip()  # å®‰å…¨è·å–å¹¶å»é™¤å¤šä½™ç©ºæ ¼
        if not api_key:
            raise ValueError("API key not found in config.json.")
        return api_key
    except Exception as e:
        print(f"Error: Unable to read API key. {str(e)}")
        return ""

# å°†å›¾åƒè½¬æ¢ä¸ºPillowå›¾åƒçš„è¾…åŠ©å‡½æ•°
def images_to_pillow(image):
    from PIL import Image
    import numpy as np
    
    if image is None:
        return []
        
    # è½¬æ¢å¼ é‡ä¸ºPILå›¾åƒ
    result = []
    if len(image.shape) == 4:  # æ‰¹é‡å›¾åƒ
        for i in range(image.shape[0]):
            img = image[i].numpy()
            img = (img * 255).astype(np.uint8)
            img = Image.fromarray(img)
            result.append(img)
    else:  # å•ä¸ªå›¾åƒ
        img = image.numpy()
        img = (img * 255).astype(np.uint8)
        img = Image.fromarray(img)
        result.append(img)
    
    return result

class GeminiPromptGeneratorJT:  
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "theme": ("STRING", {
                    "default": "", 
                    "multiline": True,
                    "tooltip": "è¾“å…¥æƒ³è¦ç”Ÿæˆçš„æç¤ºè¯ä¸»é¢˜"
                }),
                "override_system_prompt": ("STRING", {
                    "default": "", 
                    "multiline": True, 
                    "tooltip": "è‡ªå®šä¹‰æç¤ºè¯ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å˜é‡ï¼š\n{theme} - å½“å‰ä¸»é¢˜\n{prompt_history}æˆ–{memory} - å†å²æç¤ºè®°å½•(å½“Memoryè®¾ç½®ä¸ºDisableæ—¶å°†æ›¿æ¢ä¸ºç©ºåˆ—è¡¨[])\n{prompt_length} - æç¤ºé•¿åº¦è®¾ç½®\n{seed} - å½“å‰ç§å­å€¼\nä¾‹å¦‚ï¼š'ä¸º{theme}ç”Ÿæˆä¸€ä¸ªå›¾åƒæç¤ºè¯ï¼Œå†å²è®°å½•ï¼š{prompt_history}'"
                }),
                "model": (["gemini-2.5-flash", "gemini-2.5-flash-lite-preview-06-17", "gemini-2.0-flash", "gemini-2.0-flash-lite", "gemini-1.5-flash", "gemini-1.5-flash-8b", "gemini-1.5-pro"], {
                    "tooltip": "é€‰æ‹©è¦ä½¿ç”¨çš„Geminiæ¨¡å‹ã€‚**å¼ºçƒˆæ¨èä½¿ç”¨æœ€æ–°æ¨¡å‹**ï¼š\n\nğŸ”¥ **æ¨èæ¨¡å‹ï¼ˆå…è´¹å¯ç”¨ï¼‰**ï¼š\nâ€¢ **gemini-2.5-flash** - æœ€æ–°æ··åˆæ¨ç†æ¨¡å‹ï¼Œæ€§ä»·æ¯”æœ€é«˜ï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç† [å…è´¹ï¼š10 RPM, 250K TPM, 250 RPD]\nâ€¢ **gemini-2.5-flash-lite** - æœ€ç»æµå®æƒ çš„æ¨¡å‹ï¼Œé€‚åˆå¤§è§„æ¨¡ä½¿ç”¨ [å…è´¹ï¼š15 RPM, 250K TPM, 1000 RPD]\nâ€¢ **gemini-2.0-flash** - å‡è¡¡çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡ [å…è´¹ï¼š15 RPM, 1M TPM, 200 RPD]\nâ€¢ **gemini-2.0-flash-lite** - è½»é‡çº§é«˜æ•ˆæ¨¡å‹ï¼Œæˆæœ¬æœ€ä½ [å…è´¹ï¼š30 RPM, 1M TPM, 200 RPD]\n\nâš ï¸ **å·²å¼ƒç”¨æ¨¡å‹ï¼ˆä¸æ¨èï¼‰**ï¼š\nâ€¢ **gemini-1.5-flash** - å·²å¼ƒç”¨ï¼Œå°†äº2025å¹´9æœˆ24æ—¥é€€å½¹ï¼Œå»ºè®®è¿ç§»åˆ°2.0-flash-lite\nâ€¢ **gemini-1.5-flash-8b** - å·²å¼ƒç”¨ï¼Œå°†äº2025å¹´9æœˆ24æ—¥é€€å½¹\nâ€¢ **gemini-1.5-pro** - å·²å¼ƒç”¨ï¼Œå°†äº2025å¹´9æœˆ24æ—¥é€€å½¹ï¼Œå»ºè®®è¿ç§»åˆ°2.5-flash\n\nğŸ’¡ **å…è´¹å±‚è¯´æ˜**ï¼šRPM=æ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼ŒTPM=æ¯åˆ†é’ŸTokenæ•°ï¼ŒRPD=æ¯æ—¥è¯·æ±‚æ•°\nğŸ“– **è¿ç§»ä¼˜åŠ¿**ï¼šæ–°æ¨¡å‹æä¾›æ›´é«˜å…è´¹é…é¢ã€æ›´å¥½æ€§èƒ½ã€æ›´ä½æˆæœ¬å’Œæ–°åŠŸèƒ½æ”¯æŒ"
                }),
                "enable_memory": ("BOOLEAN", {
                    "default": True, 
                    "tooltip": "å¯ç”¨æˆ–ç¦ç”¨å†å²è®°å¿†åŠŸèƒ½ã€‚å¯ç”¨æ—¶ä¼šè®°ä½ä¹‹å‰ç”Ÿæˆçš„æç¤ºï¼Œé¿å…é‡å¤ï¼›ç¦ç”¨æ—¶æ¯æ¬¡ç”Ÿæˆç‹¬ç«‹æç¤º"
                }),
                "prompt_length": ("INT", {
                    "default": 200, 
                    "min": 0, 
                    "max": 5000,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆæç¤ºè¯çš„é•¿åº¦(å•è¯æ•°)ã€‚è®¾ä¸º0è¡¨ç¤ºä¸é™åˆ¶é•¿åº¦"
                }),
                "seed": ("INT", {
                    "default": 0, 
                    "min": 0, 
                    "max": 0xffffffffffffffff,
                    "tooltip": "éšæœºç§å­å€¼ï¼Œç›¸åŒçš„ç§å­ä¼šäº§ç”Ÿç›¸ä¼¼çš„ç»“æœ"
                }),
                "timeout": ("INT", {
                    "default": 30, 
                    "min": 0, 
                    "max": 6000,
                    "tooltip": "APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)ã€‚å¦‚æœåœ¨æŒ‡å®šæ—¶é—´å†…æœªæ”¶åˆ°å“åº”ï¼Œå°†ä¸­æ–­è¯·æ±‚"
                })
            },
            "optional": {
                "image_1": ("IMAGE", ),
                "image_2": ("IMAGE", ),
                "image_3": ("IMAGE", )
            }
        }
    
    RETURN_TYPES = ("STRING",)
    FUNCTION = "generate_prompt"
    CATEGORY = "text/generation"
    
    def generate_prompt(self, theme, override_system_prompt, model, enable_memory, prompt_length, seed, timeout, image_1=None, image_2=None, image_3=None):
        # å°†å¸ƒå°”å€¼è½¬æ¢ä¸ºåŸæ¥çš„å­—ç¬¦ä¸²æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        memory = "Enable" if enable_memory else "Disable"
        
        # Use the seed to initialize the random number generator
        random.seed(seed)
        
        # è·Ÿè¸ªå†…å­˜æ¨¡å¼çš„å˜åŒ–å¹¶æ¸…é™¤å†å²è®°å½•
        global last_memory_mode
        if last_memory_mode is None:
            # ç¬¬ä¸€æ¬¡è¿è¡Œ
            if memory == "Disable":
                # å¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œæ—¶å°±æ˜¯Disableæ¨¡å¼ï¼Œç¡®ä¿å†å²è®°å½•ä¸ºç©º
                prompt_history.clear()
                print("First run with Memory Disable mode. History cleared.")
        elif last_memory_mode != memory:
            # å½“æ¨¡å¼å‘ç”Ÿå˜åŒ–æ—¶
            if memory == "Enable":
                # å½“ä»Disableåˆ‡æ¢åˆ°Enableæ¨¡å¼æ—¶ï¼Œæ¸…ç©ºå†å²è®°å½•
                prompt_history.clear()
                print("Memory mode changed from Disable to Enable. History cleared.")
            else:
                # å½“ä»Enableåˆ‡æ¢åˆ°Disableæ¨¡å¼æ—¶ï¼Œæ‰“å°é€šçŸ¥ä½†ä¸æ¸…ç©ºå†å²è®°å½•(å› ä¸ºdisableæ¨¡å¼ä¸ä½¿ç”¨å†å²è®°å½•)
                print("Memory mode changed from Enable to Disable. New prompts will not be recorded.")
        
        # æ›´æ–°å½“å‰çš„å†…å­˜æ¨¡å¼
        last_memory_mode = memory

        # é€šè¿‡ config.json è·å– API å¯†é’¥
        api_key = get_gemini_api_key()
        if not api_key:
            raise ComfyUIAPIError("API key is required. Please check config.json.")
        
        # ä½¿ç”¨çº¿ç¨‹çº§åˆ«çš„å–æ¶ˆæ ‡å¿—
        cancel_event = threading.Event()
        result = [None]  # ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨ç»“æœ
        error = [None]   # ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨é”™è¯¯

        def generate_content_worker():
            try:
                # é…ç½®APIå¹¶åˆ›å»ºæ¨¡å‹
                genai.configure(api_key=api_key)
                
                # è®¾ç½®å®‰å…¨é…ç½®ä»¥é¿å…è¢«å®¡æ ¸è¿‡æ»¤
                safety_settings = {
                    "harassment": "block_none",
                    "hate_speech": "block_none", 
                    "sexually_explicit": "block_none",
                    "dangerous_content": "block_none"
                }
                
                # åˆ›å»ºç”Ÿæˆæ¨¡å‹å®ä¾‹å¹¶è®¾ç½®ç”Ÿæˆå‚æ•°
                generation_config = {
                    "temperature": 0.9,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 2048,
                    "response_mime_type": "text/plain"
                }
                
                try:
                    # å°è¯•ä½¿ç”¨æ–°ç‰ˆAPIæ ¼å¼
                    gemini_model = genai.GenerativeModel(
                        model_name=model,
                        generation_config=generation_config,
                        safety_settings=safety_settings
                    )
                except Exception as e:
                    # print(f"Warning: Failed to initialize with new API format: {str(e)}")
                    # print("Falling back to legacy API format")
                    
                    # é™çº§ä½¿ç”¨ä¼ ç»ŸAPIæ ¼å¼ - ä¸€äº›è¾ƒæ—§ç‰ˆæœ¬çš„åº“ä½¿ç”¨ä¸åŒçš„å®‰å…¨è®¾ç½®æ ¼å¼
                    safety_settings_legacy = [
                        {
                            "category": "HARM_CATEGORY_HARASSMENT",
                            "threshold": "BLOCK_NONE",
                        },
                        {
                            "category": "HARM_CATEGORY_HATE_SPEECH",
                            "threshold": "BLOCK_NONE",
                        },
                        {
                            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                            "threshold": "BLOCK_NONE",
                        },
                        {
                            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                            "threshold": "BLOCK_NONE",
                        },
                    ]
                    
                    gemini_model = genai.GenerativeModel(
                        model_name=model,
                        generation_config=generation_config,
                        safety_settings=safety_settings_legacy
                    )
                
                # å¤„ç†å›¾åƒè¾“å…¥
                images_to_send = []
                for img in [image_1, image_2, image_3]:
                    if img is not None:
                        images_to_send.extend(images_to_pillow(img))
                
                # æ˜¯å¦åŒ…å«å›¾åƒ
                has_images = len(images_to_send) > 0
                
                # æ ¹æ® prompt_length åŠ¨æ€è°ƒæ•´ input_prompt
                if not override_system_prompt:
                    if memory == "Enable":
                        if has_images:
                            input_prompt = f"Generate me a prompt for image generator based on the provided image(s). The theme of the prompt is {theme}. You already created those prompts: {list(prompt_history)}. Make sure you generate original prompt that describes and expands on what you see in the image(s). Think about it step by step and make some internal critique."
                        else:
                            input_prompt = f"Generate me a prompt for image generator. The theme of the prompt is {theme}. You already created those prompts: {list(prompt_history)}. Make sure you generate original prompt. Think about it step by step and make some internal critique."
                    else:  # memory == "Disable"
                        if has_images:
                            input_prompt = f"Generate me a prompt for image generator based on the provided image(s). The theme of the prompt is {theme}. Think about it step by step and make some internal critique."
                        else:
                            input_prompt = f"Generate me a prompt for image generator. The theme of the prompt is {theme}. Think about it step by step and make some internal critique."
                    
                    # åªæœ‰å½“ prompt_length ä¸ä¸º 0 æ—¶ï¼Œæ‰æ·»åŠ é•¿åº¦å’Œ prompt æ ‡ç­¾çš„é™åˆ¶
                    if prompt_length > 0:
                        input_prompt += f" You must keep the length of your generated prompt around {prompt_length} words. **You only need to output generated prompt and nothing else!**"
                    else:
                        input_prompt += f" **You only need to output generated prompt and nothing else!**"
                else:
                    # å¤„ç†è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡æ›¿æ¢
                    custom_prompt = override_system_prompt
                    
                    # æ”¯æŒå¤šç§å˜é‡æ ¼å¼çš„æ›¿æ¢
                    replacements = {
                        "{theme}": theme,
                        "{prompt_length}": str(prompt_length),
                        "{seed}": str(seed)
                    }
                    
                    # æ ¹æ®memoryçŠ¶æ€å†³å®šå†å²è®°å½•çš„å€¼
                    if memory == "Enable":
                        # å¯ç”¨å†å²è®°å½•æ—¶ï¼Œæ­£å¸¸æ›¿æ¢å†å²è®°å½•å˜é‡
                        replacements.update({
                            "{prompt_history}": str(list(prompt_history)),
                            "{list(prompt_history)}": str(list(prompt_history)),
                            "{memory}": str(list(prompt_history))
                        })
                    else:
                        # ç¦ç”¨å†å²è®°å½•æ—¶ï¼Œå†å²è®°å½•å˜é‡æ›¿æ¢ä¸ºç©ºåˆ—è¡¨
                        replacements.update({
                            "{prompt_history}": "[]",
                            "{list(prompt_history)}": "[]",
                            "{memory}": "[]"
                        })
                    
                    # æ›¿æ¢æ‰€æœ‰æ”¯æŒçš„å˜é‡
                    for placeholder, value in replacements.items():
                        custom_prompt = custom_prompt.replace(placeholder, value)
                    
                    # æ‰“å°è°ƒè¯•ä¿¡æ¯
                    print(f"Memory mode: {memory}")
                    if memory == "Disable":
                        print("å†å²è®°å½•å˜é‡è¢«æ›¿æ¢ä¸ºç©ºåˆ—è¡¨[]")
                    
                    if has_images:
                        input_prompt = f"The theme is {theme}. I'm providing you with image(s) for reference. Please generate a response strictly following the instruction: {custom_prompt}."
                    else:
                        input_prompt = f"The theme is {theme}. Please generate a response strictly following the instruction: {custom_prompt}."
                    
                    # åªæœ‰å½“ prompt_length ä¸ä¸º 0 æ—¶ï¼Œæ‰æ·»åŠ é•¿åº¦å’Œ prompt æ ‡ç­¾çš„é™åˆ¶
                    if prompt_length > 0:
                        input_prompt += f" You must keep the length of response around {prompt_length} words. **You only need to give me the answer and nothing else!**"
                    else:
                        input_prompt += f" **You only need to give me the answer and nothing else!**"

                # æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
                if cancel_event.is_set():
                    return
                
                # å‘é€è¯·æ±‚åˆ°API
                try:
                    if has_images:
                        # å‘é€æ–‡æœ¬å’Œå›¾åƒ
                        content_list = [input_prompt] + images_to_send
                        response = gemini_model.generate_content(content_list)
                    else:
                        # åªå‘é€æ–‡æœ¬
                        response = gemini_model.generate_content(input_prompt)
                except Exception as api_error:
                    error_msg = str(api_error)
                    if "400 Bad Request" in error_msg:
                        raise ComfyUIAPIError(f"API rejected the request due to invalid input. Details: {error_msg}")
                    elif "403 Forbidden" in error_msg:
                        raise ComfyUIAPIError(f"API access denied. Please check your API key and permissions. Details: {error_msg}")
                    elif "429 Too Many Requests" in error_msg:
                        raise ComfyUIAPIError(f"API rate limit exceeded. Please wait before trying again. Details: {error_msg}")
                    elif "500 Internal Server Error" in error_msg:
                        raise ComfyUIAPIError(f"Google API server error. Please try again later. Details: {error_msg}")
                    elif "Connection" in error_msg:
                        raise ComfyUIAPIError(f"Connection error. Please check your internet connection. Details: {error_msg}")
                    else:
                        raise ComfyUIAPIError(f"Failed to generate content. Details: {error_msg}")
                
                # æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
                if cancel_event.is_set():
                    return
                
                # å¤„ç†å“åº”
                if response and hasattr(response, 'text'):
                    generated_prompt = response.text.strip()
                    
                    print("----INPUT----")
                    print(input_prompt)
                    if has_images:
                        print(f"(åŒ…å« {len(images_to_send)} å¼ å›¾ç‰‡)")
                    print("----OUTPUT----")
                    print(generated_prompt)
                    # ä½¿ç”¨dequeçš„appendæ–¹æ³•ï¼Œå½“è¾¾åˆ°æœ€å¤§é•¿åº¦æ—¶ä¼šè‡ªåŠ¨ç§»é™¤æœ€æ—©çš„å…ƒç´ 
                    if memory == "Enable":  # ä»…åœ¨å¯ç”¨memoryæ—¶æ·»åŠ åˆ°å†å²è®°å½•
                        prompt_history.append(generated_prompt)
                    print("-------------")
                    
                    result[0] = generated_prompt
                else:
                    error[0] = ComfyUIAPIError("Received empty response from Gemini API.")
                
            except Exception as e:
                error[0] = ComfyUIAPIError(f"Failed to generate prompt. Details: {str(e)}")

        # åˆ›å»ºçº¿ç¨‹æ‰§è¡ŒAPIè°ƒç”¨
        worker_thread = threading.Thread(target=generate_content_worker)
        worker_thread.daemon = True
        worker_thread.start()
        
        # ç­‰å¾…çº¿ç¨‹å®Œæˆæˆ–è¶…æ—¶
        start_time = time.time()
        while worker_thread.is_alive():
            if time.time() - start_time > timeout:
                cancel_event.set()  # é€šçŸ¥çº¿ç¨‹å–æ¶ˆæ“ä½œ
                error_message = f"Prompt generation timed out after {timeout} seconds. Please check your network connection or increase the timeout value."
                raise ComfyUITimeoutError(error_message)
            time.sleep(0.1)  # çŸ­æš‚ä¼‘çœ ä»¥é¿å…CPUå ç”¨
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if error[0]:
            raise error[0]
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å¾—ç»“æœ
        if result[0] is None:
            raise ComfyUIAPIError("Failed to generate prompt. No result was returned.")
        
        return (result[0],)

# Node class mappings
NODE_CLASS_MAPPINGS = {
    "GeminiPromptGeneratorJT": GeminiPromptGeneratorJT
}

# Node display name mappings
NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiPromptGeneratorJT": "Gemini Prompt Generator-JT"
}
